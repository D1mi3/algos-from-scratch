{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data[\"data\"], data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.append(X, y[:, None], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train[:, -1].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - X.min()) / (X.max() - X.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: #0 - Loss: 1.092320196401541 - Acc: 0.33\n",
      "Epoch: #1 - Loss: 1.092106169796155 - Acc: 0.33\n",
      "Epoch: #2 - Loss: 1.0918923440020103 - Acc: 0.33\n",
      "Epoch: #3 - Loss: 1.091678693621717 - Acc: 0.33\n",
      "Epoch: #4 - Loss: 1.0914651934986819 - Acc: 0.33\n",
      "Epoch: #5 - Loss: 1.0912518187055806 - Acc: 0.33\n",
      "Epoch: #6 - Loss: 1.0910385445332993 - Acc: 0.33\n",
      "Epoch: #7 - Loss: 1.0908253464803346 - Acc: 0.33\n",
      "Epoch: #8 - Loss: 1.09061220024264 - Acc: 0.33\n",
      "Epoch: #9 - Loss: 1.0903990817039144 - Acc: 0.33\n",
      "Epoch: #10 - Loss: 1.0901859669263194 - Acc: 0.33\n",
      "Epoch: #11 - Loss: 1.0899728321416247 - Acc: 0.33\n",
      "Epoch: #12 - Loss: 1.0897596537427672 - Acc: 0.33\n",
      "Epoch: #13 - Loss: 1.089546408275827 - Acc: 0.33\n",
      "Epoch: #14 - Loss: 1.0893330724324095 - Acc: 0.33\n",
      "Epoch: #15 - Loss: 1.0891196230424296 - Acc: 0.33\n",
      "Epoch: #16 - Loss: 1.0889060370672996 - Acc: 0.33\n",
      "Epoch: #17 - Loss: 1.088692291593511 - Acc: 0.34\n",
      "Epoch: #18 - Loss: 1.0884783638266144 - Acc: 0.34\n",
      "Epoch: #19 - Loss: 1.088264231085586 - Acc: 0.34\n",
      "Epoch: #20 - Loss: 1.0880498707975923 - Acc: 0.35\n",
      "Epoch: #21 - Loss: 1.0878352604931385 - Acc: 0.35\n",
      "Epoch: #22 - Loss: 1.0876203778016085 - Acc: 0.35\n",
      "Epoch: #23 - Loss: 1.0874052004471928 - Acc: 0.37\n",
      "Epoch: #24 - Loss: 1.0871897062452027 - Acc: 0.41\n",
      "Epoch: #25 - Loss: 1.086973873098774 - Acc: 0.45\n",
      "Epoch: #26 - Loss: 1.0867576789959559 - Acc: 0.48\n",
      "Epoch: #27 - Loss: 1.0865411020071936 - Acc: 0.51\n",
      "Epoch: #28 - Loss: 1.0863241202831904 - Acc: 0.56\n",
      "Epoch: #29 - Loss: 1.0861067120531698 - Acc: 0.60\n",
      "Epoch: #30 - Loss: 1.0858888556235184 - Acc: 0.61\n",
      "Epoch: #31 - Loss: 1.0856705293768265 - Acc: 0.63\n",
      "Epoch: #32 - Loss: 1.0854517117713154 - Acc: 0.65\n",
      "Epoch: #33 - Loss: 1.0852323813406604 - Acc: 0.66\n",
      "Epoch: #34 - Loss: 1.0850125166942055 - Acc: 0.67\n",
      "Epoch: #35 - Loss: 1.084792096517575 - Acc: 0.67\n",
      "Epoch: #36 - Loss: 1.0845710995736761 - Acc: 0.67\n",
      "Epoch: #37 - Loss: 1.0843495047041016 - Acc: 0.67\n",
      "Epoch: #38 - Loss: 1.0841272908309247 - Acc: 0.67\n",
      "Epoch: #39 - Loss: 1.0839044369588957 - Acc: 0.66\n",
      "Epoch: #40 - Loss: 1.083680922178031 - Acc: 0.66\n",
      "Epoch: #41 - Loss: 1.0834567256666032 - Acc: 0.66\n",
      "Epoch: #42 - Loss: 1.083231826694526 - Acc: 0.66\n",
      "Epoch: #43 - Loss: 1.0830062046271363 - Acc: 0.66\n",
      "Epoch: #44 - Loss: 1.0827798389293746 - Acc: 0.66\n",
      "Epoch: #45 - Loss: 1.0825527091703582 - Acc: 0.66\n",
      "Epoch: #46 - Loss: 1.0823247950283494 - Acc: 0.66\n",
      "Epoch: #47 - Loss: 1.0820960762961176 - Acc: 0.66\n",
      "Epoch: #48 - Loss: 1.0818665328866892 - Acc: 0.66\n",
      "Epoch: #49 - Loss: 1.0816361448394884 - Acc: 0.66\n",
      "Epoch: #50 - Loss: 1.0814048923268622 - Acc: 0.66\n",
      "Epoch: #51 - Loss: 1.0811727556609871 - Acc: 0.66\n",
      "Epoch: #52 - Loss: 1.0809397153011524 - Acc: 0.66\n",
      "Epoch: #53 - Loss: 1.0807057518614236 - Acc: 0.66\n",
      "Epoch: #54 - Loss: 1.0804708461186658 - Acc: 0.66\n",
      "Epoch: #55 - Loss: 1.0802349790209376 - Acc: 0.66\n",
      "Epoch: #56 - Loss: 1.0799981316962373 - Acc: 0.67\n",
      "Epoch: #57 - Loss: 1.0797602854616024 - Acc: 0.67\n",
      "Epoch: #58 - Loss: 1.0795214218325517 - Acc: 0.67\n",
      "Epoch: #59 - Loss: 1.0792815225328594 - Acc: 0.69\n",
      "Epoch: #60 - Loss: 1.0790405695046559 - Acc: 0.69\n",
      "Epoch: #61 - Loss: 1.078798544918846 - Acc: 0.69\n",
      "Epoch: #62 - Loss: 1.0785554311858279 - Acc: 0.69\n",
      "Epoch: #63 - Loss: 1.078311210966509 - Acc: 0.69\n",
      "Epoch: #64 - Loss: 1.0780658671836003 - Acc: 0.69\n",
      "Epoch: #65 - Loss: 1.0778193830331813 - Acc: 0.69\n",
      "Epoch: #66 - Loss: 1.0775717419965125 - Acc: 0.70\n",
      "Epoch: #67 - Loss: 1.0773229278520948 - Acc: 0.70\n",
      "Epoch: #68 - Loss: 1.0770729246879402 - Acc: 0.71\n",
      "Epoch: #69 - Loss: 1.0768217169140573 - Acc: 0.71\n",
      "Epoch: #70 - Loss: 1.076569289275117 - Acc: 0.73\n",
      "Epoch: #71 - Loss: 1.0763156268632887 - Acc: 0.73\n",
      "Epoch: #72 - Loss: 1.0760607151312218 - Acc: 0.75\n",
      "Epoch: #73 - Loss: 1.0758045399051566 - Acc: 0.76\n",
      "Epoch: #74 - Loss: 1.0755470873981359 - Acc: 0.76\n",
      "Epoch: #75 - Loss: 1.075288344223299 - Acc: 0.76\n",
      "Epoch: #76 - Loss: 1.0750282974072325 - Acc: 0.77\n",
      "Epoch: #77 - Loss: 1.0747669344033528 - Acc: 0.78\n",
      "Epoch: #78 - Loss: 1.0745042431052918 - Acc: 0.79\n",
      "Epoch: #79 - Loss: 1.074240211860265 - Acc: 0.80\n",
      "Epoch: #80 - Loss: 1.0739748294823845 - Acc: 0.83\n",
      "Epoch: #81 - Loss: 1.0737080852658991 - Acc: 0.83\n",
      "Epoch: #82 - Loss: 1.0734399689983232 - Acc: 0.83\n",
      "Epoch: #83 - Loss: 1.0731704709734264 - Acc: 0.84\n",
      "Epoch: #84 - Loss: 1.0728995820040559 - Acc: 0.84\n",
      "Epoch: #85 - Loss: 1.0726272934347574 - Acc: 0.85\n",
      "Epoch: #86 - Loss: 1.0723535971541598 - Acc: 0.87\n",
      "Epoch: #87 - Loss: 1.072078485607098 - Acc: 0.88\n",
      "Epoch: #88 - Loss: 1.0718019518064295 - Acc: 0.89\n",
      "Epoch: #89 - Loss: 1.0715239893445274 - Acc: 0.90\n",
      "Epoch: #90 - Loss: 1.0712445924043967 - Acc: 0.90\n",
      "Epoch: #91 - Loss: 1.0709637557703975 - Acc: 0.92\n",
      "Epoch: #92 - Loss: 1.0706814748385314 - Acc: 0.91\n",
      "Epoch: #93 - Loss: 1.0703977456262581 - Acc: 0.92\n",
      "Epoch: #94 - Loss: 1.0701125647818146 - Acc: 0.90\n",
      "Epoch: #95 - Loss: 1.0698259295929968 - Acc: 0.91\n",
      "Epoch: #96 - Loss: 1.0695378379953755 - Acc: 0.91\n",
      "Epoch: #97 - Loss: 1.0692482885799146 - Acc: 0.91\n",
      "Epoch: #98 - Loss: 1.0689572805999568 - Acc: 0.91\n",
      "Epoch: #99 - Loss: 1.0686648139775519 - Acc: 0.91\n",
      "Epoch: #100 - Loss: 1.068370889309095 - Acc: 0.91\n",
      "Epoch: #101 - Loss: 1.0680755078702466 - Acc: 0.89\n",
      "Epoch: #102 - Loss: 1.0677786716201088 - Acc: 0.89\n",
      "Epoch: #103 - Loss: 1.0674803832046367 - Acc: 0.89\n",
      "Epoch: #104 - Loss: 1.0671806459592494 - Acc: 0.89\n",
      "Epoch: #105 - Loss: 1.0668794639106356 - Acc: 0.88\n",
      "Epoch: #106 - Loss: 1.0665768417777195 - Acc: 0.87\n",
      "Epoch: #107 - Loss: 1.0662727849717764 - Acc: 0.86\n",
      "Epoch: #108 - Loss: 1.0659672995956813 - Acc: 0.85\n",
      "Epoch: #109 - Loss: 1.0656603924422767 - Acc: 0.85\n",
      "Epoch: #110 - Loss: 1.0653520709918476 - Acc: 0.85\n",
      "Epoch: #111 - Loss: 1.065042343408696 - Acc: 0.85\n",
      "Epoch: #112 - Loss: 1.0647312185368099 - Acc: 0.83\n",
      "Epoch: #113 - Loss: 1.0644187058946204 - Acc: 0.83\n",
      "Epoch: #114 - Loss: 1.0641048156688497 - Acc: 0.83\n",
      "Epoch: #115 - Loss: 1.0637895587074502 - Acc: 0.83\n",
      "Epoch: #116 - Loss: 1.0634729465116364 - Acc: 0.81\n",
      "Epoch: #117 - Loss: 1.063154991227027 - Acc: 0.81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: #118 - Loss: 1.0628357056338942 - Acc: 0.79\n",
      "Epoch: #119 - Loss: 1.062515103136545 - Acc: 0.76\n",
      "Epoch: #120 - Loss: 1.0621931977518446 - Acc: 0.75\n",
      "Epoch: #121 - Loss: 1.0618700040969016 - Acc: 0.75\n",
      "Epoch: #122 - Loss: 1.0615455373759386 - Acc: 0.74\n",
      "Epoch: #123 - Loss: 1.0612198133663697 - Acc: 0.74\n",
      "Epoch: #124 - Loss: 1.060892848404116 - Acc: 0.74\n",
      "Epoch: #125 - Loss: 1.0605646593681823 - Acc: 0.74\n",
      "Epoch: #126 - Loss: 1.0602352636645331 - Acc: 0.73\n",
      "Epoch: #127 - Loss: 1.0599046792092994 - Acc: 0.73\n",
      "Epoch: #128 - Loss: 1.0595729244113499 - Acc: 0.73\n",
      "Epoch: #129 - Loss: 1.0592400181542723 - Acc: 0.72\n",
      "Epoch: #130 - Loss: 1.0589059797777949 - Acc: 0.72\n",
      "Epoch: #131 - Loss: 1.0585708290586997 - Acc: 0.71\n",
      "Epoch: #132 - Loss: 1.0582345861912656 - Acc: 0.71\n",
      "Epoch: #133 - Loss: 1.0578972717672845 - Acc: 0.71\n",
      "Epoch: #134 - Loss: 1.0575589067557039 - Acc: 0.71\n",
      "Epoch: #135 - Loss: 1.0572195124819328 - Acc: 0.70\n",
      "Epoch: #136 - Loss: 1.0568791106068678 - Acc: 0.69\n",
      "Epoch: #137 - Loss: 1.05653772310568 - Acc: 0.69\n",
      "Epoch: #138 - Loss: 1.056195372246416 - Acc: 0.69\n",
      "Epoch: #139 - Loss: 1.055852080568461 - Acc: 0.69\n",
      "Epoch: #140 - Loss: 1.05550787086091 - Acc: 0.69\n",
      "Epoch: #141 - Loss: 1.0551627661408982 - Acc: 0.68\n",
      "Epoch: #142 - Loss: 1.0548167896319356 - Acc: 0.68\n",
      "Epoch: #143 - Loss: 1.0544699647422962 - Acc: 0.68\n",
      "Epoch: #144 - Loss: 1.0541223150435064 - Acc: 0.67\n",
      "Epoch: #145 - Loss: 1.0537738642489762 - Acc: 0.67\n",
      "Epoch: #146 - Loss: 1.053424636192821 - Acc: 0.67\n",
      "Epoch: #147 - Loss: 1.0530746548089114 - Acc: 0.67\n",
      "Epoch: #148 - Loss: 1.052723944110199 - Acc: 0.67\n",
      "Epoch: #149 - Loss: 1.0523725281683516 - Acc: 0.67\n",
      "Epoch: #150 - Loss: 1.052020431093737 - Acc: 0.67\n",
      "Epoch: #151 - Loss: 1.0516676770157949 - Acc: 0.68\n",
      "Epoch: #152 - Loss: 1.0513142900638233 - Acc: 0.68\n",
      "Epoch: #153 - Loss: 1.0509602943482188 - Acc: 0.68\n",
      "Epoch: #154 - Loss: 1.0506057139421963 - Acc: 0.68\n",
      "Epoch: #155 - Loss: 1.050250572864013 - Acc: 0.68\n",
      "Epoch: #156 - Loss: 1.0498948950597273 - Acc: 0.68\n",
      "Epoch: #157 - Loss: 1.049538704386511 - Acc: 0.68\n",
      "Epoch: #158 - Loss: 1.0491820245965342 - Acc: 0.68\n",
      "Epoch: #159 - Loss: 1.048824879321447 - Acc: 0.68\n",
      "Epoch: #160 - Loss: 1.0484672920574634 - Acc: 0.69\n",
      "Epoch: #161 - Loss: 1.0481092861510712 - Acc: 0.69\n",
      "Epoch: #162 - Loss: 1.0477508847853692 - Acc: 0.71\n",
      "Epoch: #163 - Loss: 1.047392110967047 - Acc: 0.71\n",
      "Epoch: #164 - Loss: 1.047032987514011 - Acc: 0.71\n",
      "Epoch: #165 - Loss: 1.046673537043659 - Acc: 0.71\n",
      "Epoch: #166 - Loss: 1.0463137819618085 - Acc: 0.71\n",
      "Epoch: #167 - Loss: 1.0459537444522764 - Acc: 0.71\n",
      "Epoch: #168 - Loss: 1.0455934464671088 - Acc: 0.71\n",
      "Epoch: #169 - Loss: 1.0452329097174569 - Acc: 0.71\n",
      "Epoch: #170 - Loss: 1.0448721556650915 - Acc: 0.71\n",
      "Epoch: #171 - Loss: 1.044511205514555 - Acc: 0.71\n",
      "Epoch: #172 - Loss: 1.0441500802059296 - Acc: 0.71\n",
      "Epoch: #173 - Loss: 1.043788800408227 - Acc: 0.71\n",
      "Epoch: #174 - Loss: 1.0434273865133725 - Acc: 0.71\n",
      "Epoch: #175 - Loss: 1.043065858630782 - Acc: 0.71\n",
      "Epoch: #176 - Loss: 1.0427042365825132 - Acc: 0.71\n",
      "Epoch: #177 - Loss: 1.0423425398989699 - Acc: 0.71\n",
      "Epoch: #178 - Loss: 1.0419807878151581 - Acc: 0.71\n",
      "Epoch: #179 - Loss: 1.0416189992674598 - Acc: 0.72\n",
      "Epoch: #180 - Loss: 1.0412571928909184 - Acc: 0.72\n",
      "Epoch: #181 - Loss: 1.0408953870170126 - Acc: 0.72\n",
      "Epoch: #182 - Loss: 1.0405335996719036 - Acc: 0.72\n",
      "Epoch: #183 - Loss: 1.0401718485751288 - Acc: 0.73\n",
      "Epoch: #184 - Loss: 1.0398101511387328 - Acc: 0.73\n",
      "Epoch: #185 - Loss: 1.0394485244668068 - Acc: 0.73\n",
      "Epoch: #186 - Loss: 1.0390869853554223 - Acc: 0.73\n",
      "Epoch: #187 - Loss: 1.0387255502929387 - Acc: 0.73\n",
      "Epoch: #188 - Loss: 1.0383642354606586 - Acc: 0.73\n",
      "Epoch: #189 - Loss: 1.038003056733819 - Acc: 0.73\n",
      "Epoch: #190 - Loss: 1.0376420296828976 - Acc: 0.73\n",
      "Epoch: #191 - Loss: 1.0372811695752095 - Acc: 0.74\n",
      "Epoch: #192 - Loss: 1.0369204913767762 - Acc: 0.74\n",
      "Epoch: #193 - Loss: 1.0365600097544578 - Acc: 0.74\n",
      "Epoch: #194 - Loss: 1.036199739078316 - Acc: 0.75\n",
      "Epoch: #195 - Loss: 1.0358396934241971 - Acc: 0.75\n",
      "Epoch: #196 - Loss: 1.035479886576521 - Acc: 0.75\n",
      "Epoch: #197 - Loss: 1.0351203320312488 - Acc: 0.75\n",
      "Epoch: #198 - Loss: 1.0347610429990255 - Acc: 0.75\n",
      "Epoch: #199 - Loss: 1.0344020324084693 - Acc: 0.75\n",
      "Epoch: #200 - Loss: 1.0340433129095996 - Acc: 0.76\n",
      "Epoch: #201 - Loss: 1.033684896877391 - Acc: 0.77\n",
      "Epoch: #202 - Loss: 1.0333267964154291 - Acc: 0.77\n",
      "Epoch: #203 - Loss: 1.0329690233596651 - Acc: 0.77\n",
      "Epoch: #204 - Loss: 1.0326115892822525 - Acc: 0.78\n",
      "Epoch: #205 - Loss: 1.03225450549545 - Acc: 0.78\n",
      "Epoch: #206 - Loss: 1.0318977830555836 - Acc: 0.78\n",
      "Epoch: #207 - Loss: 1.0315414327670573 - Acc: 0.78\n",
      "Epoch: #208 - Loss: 1.031185465186398 - Acc: 0.79\n",
      "Epoch: #209 - Loss: 1.0308298906263251 - Acc: 0.79\n",
      "Epoch: #210 - Loss: 1.0304747191598416 - Acc: 0.79\n",
      "Epoch: #211 - Loss: 1.03011996062433 - Acc: 0.79\n",
      "Epoch: #212 - Loss: 1.029765624625652 - Acc: 0.79\n",
      "Epoch: #213 - Loss: 1.0294117205422366 - Acc: 0.79\n",
      "Epoch: #214 - Loss: 1.02905825752916 - Acc: 0.79\n",
      "Epoch: #215 - Loss: 1.0287052445221991 - Acc: 0.79\n",
      "Epoch: #216 - Loss: 1.028352690241865 - Acc: 0.80\n",
      "Epoch: #217 - Loss: 1.0280006031973976 - Acc: 0.80\n",
      "Epoch: #218 - Loss: 1.0276489916907325 - Acc: 0.80\n",
      "Epoch: #219 - Loss: 1.0272978638204175 - Acc: 0.80\n",
      "Epoch: #220 - Loss: 1.02694722748549 - Acc: 0.80\n",
      "Epoch: #221 - Loss: 1.0265970903893065 - Acc: 0.80\n",
      "Epoch: #222 - Loss: 1.026247460043312 - Acc: 0.80\n",
      "Epoch: #223 - Loss: 1.025898343770767 - Acc: 0.80\n",
      "Epoch: #224 - Loss: 1.0255497487104053 - Acc: 0.80\n",
      "Epoch: #225 - Loss: 1.025201681820041 - Acc: 0.80\n",
      "Epoch: #226 - Loss: 1.0248541498801091 - Acc: 0.80\n",
      "Epoch: #227 - Loss: 1.0245071594971467 - Acc: 0.80\n",
      "Epoch: #228 - Loss: 1.024160717107207 - Acc: 0.81\n",
      "Epoch: #229 - Loss: 1.0238148289792124 - Acc: 0.81\n",
      "Epoch: #230 - Loss: 1.0234695012182378 - Acc: 0.82\n",
      "Epoch: #231 - Loss: 1.0231247397687313 - Acc: 0.82\n",
      "Epoch: #232 - Loss: 1.022780550417666 - Acc: 0.83\n",
      "Epoch: #233 - Loss: 1.0224369387976255 - Acc: 0.83\n",
      "Epoch: #234 - Loss: 1.0220939103898263 - Acc: 0.83\n",
      "Epoch: #235 - Loss: 1.021751470527069 - Acc: 0.83\n",
      "Epoch: #236 - Loss: 1.021409624396628 - Acc: 0.83\n",
      "Epoch: #237 - Loss: 1.0210683770430735 - Acc: 0.83\n",
      "Epoch: #238 - Loss: 1.0207277333710305 - Acc: 0.84\n",
      "Epoch: #239 - Loss: 1.020387698147874 - Acc: 0.84\n",
      "Epoch: #240 - Loss: 1.0200482760063598 - Acc: 0.84\n",
      "Epoch: #241 - Loss: 1.0197094714471973 - Acc: 0.84\n",
      "Epoch: #242 - Loss: 1.0193712888415571 - Acc: 0.84\n",
      "Epoch: #243 - Loss: 1.0190337324335224 - Acc: 0.85\n",
      "Epoch: #244 - Loss: 1.0186968063424802 - Acc: 0.85\n",
      "Epoch: #245 - Loss: 1.0183605145654568 - Acc: 0.85\n",
      "Epoch: #246 - Loss: 1.0180248609793958 - Acc: 0.86\n",
      "Epoch: #247 - Loss: 1.0176898493433855 - Acc: 0.87\n",
      "Epoch: #248 - Loss: 1.0173554833008258 - Acc: 0.87\n",
      "Epoch: #249 - Loss: 1.0170217663815517 - Acc: 0.87\n",
      "Epoch: #250 - Loss: 1.0166887020039015 - Acc: 0.87\n",
      "Epoch: #251 - Loss: 1.0163562934767367 - Acc: 0.87\n",
      "Epoch: #252 - Loss: 1.0160245440014166 - Acc: 0.87\n",
      "Epoch: #253 - Loss: 1.0156934566737235 - Acc: 0.87\n",
      "Epoch: #254 - Loss: 1.0153630344857454 - Acc: 0.87\n",
      "Epoch: #255 - Loss: 1.015033280327715 - Acc: 0.87\n",
      "Epoch: #256 - Loss: 1.0147041969898059 - Acc: 0.87\n",
      "Epoch: #257 - Loss: 1.0143757871638903 - Acc: 0.87\n",
      "Epoch: #258 - Loss: 1.0140480534452545 - Acc: 0.87\n",
      "Epoch: #259 - Loss: 1.0137209983342776 - Acc: 0.87\n",
      "Epoch: #260 - Loss: 1.0133946242380762 - Acc: 0.87\n",
      "Epoch: #261 - Loss: 1.0130689334721092 - Acc: 0.87\n",
      "Epoch: #262 - Loss: 1.0127439282617505 - Acc: 0.87\n",
      "Epoch: #263 - Loss: 1.0124196107438306 - Acc: 0.88\n",
      "Epoch: #264 - Loss: 1.0120959829681437 - Acc: 0.88\n",
      "Epoch: #265 - Loss: 1.0117730468989254 - Acc: 0.88\n",
      "Epoch: #266 - Loss: 1.0114508044163 - Acc: 0.88\n",
      "Epoch: #267 - Loss: 1.0111292573177013 - Acc: 0.88\n",
      "Epoch: #268 - Loss: 1.0108084073192645 - Acc: 0.88\n",
      "Epoch: #269 - Loss: 1.01048825605719 - Acc: 0.88\n",
      "Epoch: #270 - Loss: 1.0101688050890876 - Acc: 0.88\n",
      "Epoch: #271 - Loss: 1.0098500558952885 - Acc: 0.89\n",
      "Epoch: #272 - Loss: 1.0095320098801412 - Acc: 0.89\n",
      "Epoch: #273 - Loss: 1.009214668373278 - Acc: 0.89\n",
      "Epoch: #274 - Loss: 1.0088980326308639 - Acc: 0.89\n",
      "Epoch: #275 - Loss: 1.008582103836825 - Acc: 0.89\n",
      "Epoch: #276 - Loss: 1.008266883104053 - Acc: 0.89\n",
      "Epoch: #277 - Loss: 1.007952371475592 - Acc: 0.89\n",
      "Epoch: #278 - Loss: 1.0076385699258075 - Acc: 0.90\n",
      "Epoch: #279 - Loss: 1.0073254793615363 - Acc: 0.90\n",
      "Epoch: #280 - Loss: 1.0070131006232175 - Acc: 0.90\n",
      "Epoch: #281 - Loss: 1.00670143448601 - Acc: 0.90\n",
      "Epoch: #282 - Loss: 1.0063904816608877 - Acc: 0.90\n",
      "Epoch: #283 - Loss: 1.0060802427957276 - Acc: 0.91\n",
      "Epoch: #284 - Loss: 1.005770718476373 - Acc: 0.91\n",
      "Epoch: #285 - Loss: 1.005461909227689 - Acc: 0.91\n",
      "Epoch: #286 - Loss: 1.0051538155146014 - Acc: 0.91\n",
      "Epoch: #287 - Loss: 1.0048464377431192 - Acc: 0.91\n",
      "Epoch: #288 - Loss: 1.0045397762613486 - Acc: 0.91\n",
      "Epoch: #289 - Loss: 1.0042338313604886 - Acc: 0.91\n",
      "Epoch: #290 - Loss: 1.0039286032758183 - Acc: 0.91\n",
      "Epoch: #291 - Loss: 1.00362409218767 - Acc: 0.91\n",
      "Epoch: #292 - Loss: 1.0033202982223883 - Acc: 0.91\n",
      "Epoch: #293 - Loss: 1.003017221453281 - Acc: 0.91\n",
      "Epoch: #294 - Loss: 1.0027148619015567 - Acc: 0.91\n",
      "Epoch: #295 - Loss: 1.0024132195372506 - Acc: 0.91\n",
      "Epoch: #296 - Loss: 1.0021122942801417 - Acc: 0.91\n",
      "Epoch: #297 - Loss: 1.0018120860006563 - Acc: 0.91\n",
      "Epoch: #298 - Loss: 1.0015125945207624 - Acc: 0.91\n",
      "Epoch: #299 - Loss: 1.0012138196148557 - Acc: 0.91\n",
      "Epoch: #300 - Loss: 1.000915761010632 - Acc: 0.91\n",
      "Epoch: #301 - Loss: 1.0006184183899534 - Acc: 0.91\n",
      "Epoch: #302 - Loss: 1.0003217913896991 - Acc: 0.91\n",
      "Epoch: #303 - Loss: 1.0000258796026142 - Acc: 0.91\n",
      "Epoch: #304 - Loss: 0.9997306825781448 - Acc: 0.91\n",
      "Epoch: #305 - Loss: 0.9994361998232613 - Acc: 0.91\n",
      "Epoch: #306 - Loss: 0.9991424308032775 - Acc: 0.91\n",
      "Epoch: #307 - Loss: 0.998849374942659 - Acc: 0.91\n",
      "Epoch: #308 - Loss: 0.9985570316258205 - Acc: 0.92\n",
      "Epoch: #309 - Loss: 0.9982654001979173 - Acc: 0.92\n",
      "Epoch: #310 - Loss: 0.9979744799656252 - Acc: 0.92\n",
      "Epoch: #311 - Loss: 0.9976842701979145 - Acc: 0.92\n",
      "Epoch: #312 - Loss: 0.997394770126812 - Acc: 0.92\n",
      "Epoch: #313 - Loss: 0.9971059789481584 - Acc: 0.92\n",
      "Epoch: #314 - Loss: 0.9968178958223537 - Acc: 0.92\n",
      "Epoch: #315 - Loss: 0.9965305198750962 - Acc: 0.92\n",
      "Epoch: #316 - Loss: 0.9962438501981122 - Acc: 0.92\n",
      "Epoch: #317 - Loss: 0.9959578858498775 - Acc: 0.92\n",
      "Epoch: #318 - Loss: 0.9956726258563314 - Acc: 0.93\n",
      "Epoch: #319 - Loss: 0.9953880692115805 - Acc: 0.93\n",
      "Epoch: #320 - Loss: 0.9951042148785977 - Acc: 0.93\n",
      "Epoch: #321 - Loss: 0.9948210617899084 - Acc: 0.93\n",
      "Epoch: #322 - Loss: 0.9945386088482732 - Acc: 0.93\n",
      "Epoch: #323 - Loss: 0.9942568549273579 - Acc: 0.93\n",
      "Epoch: #324 - Loss: 0.9939757988724013 - Acc: 0.93\n",
      "Epoch: #325 - Loss: 0.9936954395008681 - Acc: 0.93\n",
      "Epoch: #326 - Loss: 0.9934157756030991 - Acc: 0.93\n",
      "Epoch: #327 - Loss: 0.9931368059429512 - Acc: 0.93\n",
      "Epoch: #328 - Loss: 0.9928585292584292 - Acc: 0.93\n",
      "Epoch: #329 - Loss: 0.992580944262311 - Acc: 0.93\n",
      "Epoch: #330 - Loss: 0.992304049642764 - Acc: 0.93\n",
      "Epoch: #331 - Loss: 0.992027844063952 - Acc: 0.93\n",
      "Epoch: #332 - Loss: 0.9917523261666393 - Acc: 0.93\n",
      "Epoch: #333 - Loss: 0.9914774945687801 - Acc: 0.93\n",
      "Epoch: #334 - Loss: 0.9912033478661061 - Acc: 0.93\n",
      "Epoch: #335 - Loss: 0.9909298846327032 - Acc: 0.93\n",
      "Epoch: #336 - Loss: 0.9906571034215795 - Acc: 0.93\n",
      "Epoch: #337 - Loss: 0.99038500276523 - Acc: 0.93\n",
      "Epoch: #338 - Loss: 0.9901135811761886 - Acc: 0.93\n",
      "Epoch: #339 - Loss: 0.9898428371475755 - Acc: 0.93\n",
      "Epoch: #340 - Loss: 0.9895727691536358 - Acc: 0.93\n",
      "Epoch: #341 - Loss: 0.9893033756502708 - Acc: 0.93\n",
      "Epoch: #342 - Loss: 0.9890346550755623 - Acc: 0.93\n",
      "Epoch: #343 - Loss: 0.9887666058502875 - Acc: 0.93\n",
      "Epoch: #344 - Loss: 0.9884992263784295 - Acc: 0.93\n",
      "Epoch: #345 - Loss: 0.9882325150476765 - Acc: 0.93\n",
      "Epoch: #346 - Loss: 0.9879664702299177 - Acc: 0.93\n",
      "Epoch: #347 - Loss: 0.9877010902817273 - Acc: 0.93\n",
      "Epoch: #348 - Loss: 0.9874363735448458 - Acc: 0.93\n",
      "Epoch: #349 - Loss: 0.9871723183466504 - Acc: 0.93\n",
      "Epoch: #350 - Loss: 0.9869089230006205 - Acc: 0.93\n",
      "Epoch: #351 - Loss: 0.9866461858067932 - Acc: 0.93\n",
      "Epoch: #352 - Loss: 0.986384105052216 - Acc: 0.94\n",
      "Epoch: #353 - Loss: 0.986122679011387 - Acc: 0.94\n",
      "Epoch: #354 - Loss: 0.9858619059466928 - Acc: 0.94\n",
      "Epoch: #355 - Loss: 0.9856017841088363 - Acc: 0.94\n",
      "Epoch: #356 - Loss: 0.9853423117372592 - Acc: 0.94\n",
      "Epoch: #357 - Loss: 0.9850834870605563 - Acc: 0.94\n",
      "Epoch: #358 - Loss: 0.9848253082968832 - Acc: 0.94\n",
      "Epoch: #359 - Loss: 0.9845677736543591 - Acc: 0.94\n",
      "Epoch: #360 - Loss: 0.9843108813314592 - Acc: 0.94\n",
      "Epoch: #361 - Loss: 0.9840546295174041 - Acc: 0.95\n",
      "Epoch: #362 - Loss: 0.9837990163925386 - Acc: 0.95\n",
      "Epoch: #363 - Loss: 0.9835440401287093 - Acc: 0.95\n",
      "Epoch: #364 - Loss: 0.9832896988896292 - Acc: 0.95\n",
      "Epoch: #365 - Loss: 0.9830359908312406 - Acc: 0.95\n",
      "Epoch: #366 - Loss: 0.9827829141020711 - Acc: 0.95\n",
      "Epoch: #367 - Loss: 0.9825304668435786 - Acc: 0.95\n",
      "Epoch: #368 - Loss: 0.9822786471904968 - Acc: 0.95\n",
      "Epoch: #369 - Loss: 0.982027453271171 - Acc: 0.95\n",
      "Epoch: #370 - Loss: 0.9817768832078853 - Acc: 0.95\n",
      "Epoch: #371 - Loss: 0.9815269351171881 - Acc: 0.95\n",
      "Epoch: #372 - Loss: 0.9812776071102098 - Acc: 0.95\n",
      "Epoch: #373 - Loss: 0.9810288972929732 - Acc: 0.95\n",
      "Epoch: #374 - Loss: 0.9807808037666996 - Acc: 0.95\n",
      "Epoch: #375 - Loss: 0.9805333246281096 - Acc: 0.95\n",
      "Epoch: #376 - Loss: 0.9802864579697147 - Acc: 0.95\n",
      "Epoch: #377 - Loss: 0.9800402018801071 - Acc: 0.95\n",
      "Epoch: #378 - Loss: 0.979794554444242 - Acc: 0.95\n",
      "Epoch: #379 - Loss: 0.9795495137437134 - Acc: 0.95\n",
      "Epoch: #380 - Loss: 0.9793050778570277 - Acc: 0.95\n",
      "Epoch: #381 - Loss: 0.9790612448598666 - Acc: 0.95\n",
      "Epoch: #382 - Loss: 0.9788180128253497 - Acc: 0.95\n",
      "Epoch: #383 - Loss: 0.9785753798242884 - Acc: 0.95\n",
      "Epoch: #384 - Loss: 0.9783333439254364 - Acc: 0.95\n",
      "Epoch: #385 - Loss: 0.9780919031957341 - Acc: 0.95\n",
      "Epoch: #386 - Loss: 0.9778510557005476 - Acc: 0.95\n",
      "Epoch: #387 - Loss: 0.9776107995039035 - Acc: 0.95\n",
      "Epoch: #388 - Loss: 0.9773711326687188 - Acc: 0.95\n",
      "Epoch: #389 - Loss: 0.9771320532570232 - Acc: 0.95\n",
      "Epoch: #390 - Loss: 0.9768935593301815 - Acc: 0.95\n",
      "Epoch: #391 - Loss: 0.9766556489491067 - Acc: 0.95\n",
      "Epoch: #392 - Loss: 0.9764183201744694 - Acc: 0.95\n",
      "Epoch: #393 - Loss: 0.9761815710669055 - Acc: 0.95\n",
      "Epoch: #394 - Loss: 0.9759453996872139 - Acc: 0.95\n",
      "Epoch: #395 - Loss: 0.9757098040965555 - Acc: 0.95\n",
      "Epoch: #396 - Loss: 0.975474782356643 - Acc: 0.95\n",
      "Epoch: #397 - Loss: 0.9752403325299293 - Acc: 0.95\n",
      "Epoch: #398 - Loss: 0.9750064526797894 - Acc: 0.95\n",
      "Epoch: #399 - Loss: 0.9747731408707008 - Acc: 0.95\n",
      "Epoch: #400 - Loss: 0.9745403951684157 - Acc: 0.95\n",
      "Epoch: #401 - Loss: 0.9743082136401343 - Acc: 0.95\n",
      "Epoch: #402 - Loss: 0.9740765943546679 - Acc: 0.95\n",
      "Epoch: #403 - Loss: 0.9738455353826035 - Acc: 0.95\n",
      "Epoch: #404 - Loss: 0.9736150347964618 - Acc: 0.95\n",
      "Epoch: #405 - Loss: 0.9733850906708498 - Acc: 0.95\n",
      "Epoch: #406 - Loss: 0.9731557010826137 - Acc: 0.95\n",
      "Epoch: #407 - Loss: 0.9729268641109851 - Acc: 0.95\n",
      "Epoch: #408 - Loss: 0.9726985778377232 - Acc: 0.95\n",
      "Epoch: #409 - Loss: 0.9724708403472548 - Acc: 0.95\n",
      "Epoch: #410 - Loss: 0.9722436497268117 - Acc: 0.95\n",
      "Epoch: #411 - Loss: 0.9720170040665598 - Acc: 0.95\n",
      "Epoch: #412 - Loss: 0.9717909014597316 - Acc: 0.95\n",
      "Epoch: #413 - Loss: 0.9715653400027497 - Acc: 0.95\n",
      "Epoch: #414 - Loss: 0.9713403177953494 - Acc: 0.95\n",
      "Epoch: #415 - Loss: 0.9711158329406976 - Acc: 0.95\n",
      "Epoch: #416 - Loss: 0.9708918835455089 - Acc: 0.95\n",
      "Epoch: #417 - Loss: 0.9706684677201571 - Acc: 0.95\n",
      "Epoch: #418 - Loss: 0.970445583578786 - Acc: 0.95\n",
      "Epoch: #419 - Loss: 0.9702232292394148 - Acc: 0.95\n",
      "Epoch: #420 - Loss: 0.9700014028240419 - Acc: 0.95\n",
      "Epoch: #421 - Loss: 0.9697801024587442 - Acc: 0.95\n",
      "Epoch: #422 - Loss: 0.9695593262737767 - Acc: 0.95\n",
      "Epoch: #423 - Loss: 0.9693390724036649 - Acc: 0.95\n",
      "Epoch: #424 - Loss: 0.9691193389872979 - Acc: 0.95\n",
      "Epoch: #425 - Loss: 0.9689001241680175 - Acc: 0.95\n",
      "Epoch: #426 - Loss: 0.9686814260937039 - Acc: 0.95\n",
      "Epoch: #427 - Loss: 0.9684632429168611 - Acc: 0.95\n",
      "Epoch: #428 - Loss: 0.9682455727946954 - Acc: 0.95\n",
      "Epoch: #429 - Loss: 0.9680284138891968 - Acc: 0.95\n",
      "Epoch: #430 - Loss: 0.9678117643672137 - Acc: 0.95\n",
      "Epoch: #431 - Loss: 0.9675956224005268 - Acc: 0.95\n",
      "Epoch: #432 - Loss: 0.9673799861659207 - Acc: 0.95\n",
      "Epoch: #433 - Loss: 0.9671648538452524 - Acc: 0.95\n",
      "Epoch: #434 - Loss: 0.9669502236255186 - Acc: 0.95\n",
      "Epoch: #435 - Loss: 0.9667360936989204 - Acc: 0.95\n",
      "Epoch: #436 - Loss: 0.9665224622629239 - Acc: 0.95\n",
      "Epoch: #437 - Loss: 0.9663093275203213 - Acc: 0.95\n",
      "Epoch: #438 - Loss: 0.9660966876792899 - Acc: 0.95\n",
      "Epoch: #439 - Loss: 0.9658845409534458 - Acc: 0.95\n",
      "Epoch: #440 - Loss: 0.9656728855618986 - Acc: 0.95\n",
      "Epoch: #441 - Loss: 0.9654617197293036 - Acc: 0.95\n",
      "Epoch: #442 - Loss: 0.9652510416859116 - Acc: 0.95\n",
      "Epoch: #443 - Loss: 0.965040849667616 - Acc: 0.95\n",
      "Epoch: #444 - Loss: 0.9648311419159987 - Acc: 0.95\n",
      "Epoch: #445 - Loss: 0.964621916678376 - Acc: 0.95\n",
      "Epoch: #446 - Loss: 0.9644131722078392 - Acc: 0.95\n",
      "Epoch: #447 - Loss: 0.9642049067632956 - Acc: 0.95\n",
      "Epoch: #448 - Loss: 0.9639971186095082 - Acc: 0.95\n",
      "Epoch: #449 - Loss: 0.9637898060171316 - Acc: 0.95\n",
      "Epoch: #450 - Loss: 0.9635829672627487 - Acc: 0.95\n",
      "Epoch: #451 - Loss: 0.9633766006289037 - Acc: 0.95\n",
      "Epoch: #452 - Loss: 0.9631707044041354 - Acc: 0.95\n",
      "Epoch: #453 - Loss: 0.9629652768830067 - Acc: 0.95\n",
      "Epoch: #454 - Loss: 0.962760316366135 - Acc: 0.95\n",
      "Epoch: #455 - Loss: 0.9625558211602196 - Acc: 0.95\n",
      "Epoch: #456 - Loss: 0.9623517895780676 - Acc: 0.95\n",
      "Epoch: #457 - Loss: 0.962148219938619 - Acc: 0.95\n",
      "Epoch: #458 - Loss: 0.9619451105669706 - Acc: 0.95\n",
      "Epoch: #459 - Loss: 0.9617424597943981 - Acc: 0.95\n",
      "Epoch: #460 - Loss: 0.9615402659583762 - Acc: 0.95\n",
      "Epoch: #461 - Loss: 0.9613385274025984 - Acc: 0.95\n",
      "Epoch: #462 - Loss: 0.9611372424769964 - Acc: 0.95\n",
      "Epoch: #463 - Loss: 0.960936409537755 - Acc: 0.95\n",
      "Epoch: #464 - Loss: 0.9607360269473301 - Acc: 0.95\n",
      "Epoch: #465 - Loss: 0.960536093074461 - Acc: 0.95\n",
      "Epoch: #466 - Loss: 0.9603366062941867 - Acc: 0.95\n",
      "Epoch: #467 - Loss: 0.9601375649878549 - Acc: 0.95\n",
      "Epoch: #468 - Loss: 0.959938967543136 - Acc: 0.95\n",
      "Epoch: #469 - Loss: 0.9597408123540322 - Acc: 0.95\n",
      "Epoch: #470 - Loss: 0.9595430978208863 - Acc: 0.95\n",
      "Epoch: #471 - Loss: 0.9593458223503899 - Acc: 0.95\n",
      "Epoch: #472 - Loss: 0.9591489843555909 - Acc: 0.95\n",
      "Epoch: #473 - Loss: 0.9589525822558991 - Acc: 0.95\n",
      "Epoch: #474 - Loss: 0.9587566144770907 - Acc: 0.95\n",
      "Epoch: #475 - Loss: 0.9585610794513154 - Acc: 0.95\n",
      "Epoch: #476 - Loss: 0.9583659756170952 - Acc: 0.95\n",
      "Epoch: #477 - Loss: 0.9581713014193302 - Acc: 0.95\n",
      "Epoch: #478 - Loss: 0.9579770553092996 - Acc: 0.95\n",
      "Epoch: #479 - Loss: 0.9577832357446605 - Acc: 0.95\n",
      "Epoch: #480 - Loss: 0.9575898411894502 - Acc: 0.95\n",
      "Epoch: #481 - Loss: 0.957396870114084 - Acc: 0.95\n",
      "Epoch: #482 - Loss: 0.9572043209953527 - Acc: 0.95\n",
      "Epoch: #483 - Loss: 0.9570121923164222 - Acc: 0.95\n",
      "Epoch: #484 - Loss: 0.9568204825668287 - Acc: 0.95\n",
      "Epoch: #485 - Loss: 0.9566291902424744 - Acc: 0.95\n",
      "Epoch: #486 - Loss: 0.9564383138456249 - Acc: 0.95\n",
      "Epoch: #487 - Loss: 0.956247851884901 - Acc: 0.95\n",
      "Epoch: #488 - Loss: 0.9560578028752758 - Acc: 0.95\n",
      "Epoch: #489 - Loss: 0.9558681653380652 - Acc: 0.95\n",
      "Epoch: #490 - Loss: 0.9556789378009221 - Acc: 0.95\n",
      "Epoch: #491 - Loss: 0.9554901187978279 - Acc: 0.95\n",
      "Epoch: #492 - Loss: 0.9553017068690853 - Acc: 0.95\n",
      "Epoch: #493 - Loss: 0.955113700561307 - Acc: 0.95\n",
      "Epoch: #494 - Loss: 0.9549260984274076 - Acc: 0.95\n",
      "Epoch: #495 - Loss: 0.9547388990265934 - Acc: 0.95\n",
      "Epoch: #496 - Loss: 0.9545521009243498 - Acc: 0.95\n",
      "Epoch: #497 - Loss: 0.9543657026924333 - Acc: 0.95\n",
      "Epoch: #498 - Loss: 0.9541797029088557 - Acc: 0.95\n",
      "Epoch: #499 - Loss: 0.9539941001578754 - Acc: 0.95\n",
      "Epoch: #500 - Loss: 0.9538088930299828 - Acc: 0.95\n",
      "Epoch: #501 - Loss: 0.9536240801218868 - Acc: 0.95\n",
      "Epoch: #502 - Loss: 0.9534396600365023 - Acc: 0.95\n",
      "Epoch: #503 - Loss: 0.9532556313829352 - Acc: 0.95\n",
      "Epoch: #504 - Loss: 0.9530719927764679 - Acc: 0.95\n",
      "Epoch: #505 - Loss: 0.9528887428385455 - Acc: 0.95\n",
      "Epoch: #506 - Loss: 0.9527058801967583 - Acc: 0.95\n",
      "Epoch: #507 - Loss: 0.9525234034848282 - Acc: 0.95\n",
      "Epoch: #508 - Loss: 0.9523413113425907 - Acc: 0.95\n",
      "Epoch: #509 - Loss: 0.9521596024159802 - Acc: 0.95\n",
      "Epoch: #510 - Loss: 0.9519782753570117 - Acc: 0.95\n",
      "Epoch: #511 - Loss: 0.9517973288237649 - Acc: 0.95\n",
      "Epoch: #512 - Loss: 0.9516167614803652 - Acc: 0.95\n",
      "Epoch: #513 - Loss: 0.9514365719969675 - Acc: 0.95\n",
      "Epoch: #514 - Loss: 0.9512567590497369 - Acc: 0.95\n",
      "Epoch: #515 - Loss: 0.9510773213208301 - Acc: 0.95\n",
      "Epoch: #516 - Loss: 0.9508982574983778 - Acc: 0.95\n",
      "Epoch: #517 - Loss: 0.9507195662764654 - Acc: 0.95\n",
      "Epoch: #518 - Loss: 0.9505412463551128 - Acc: 0.95\n",
      "Epoch: #519 - Loss: 0.9503632964402557 - Acc: 0.95\n",
      "Epoch: #520 - Loss: 0.9501857152437264 - Acc: 0.95\n",
      "Epoch: #521 - Loss: 0.9500085014832312 - Acc: 0.95\n",
      "Epoch: #522 - Loss: 0.9498316538823337 - Acc: 0.95\n",
      "Epoch: #523 - Loss: 0.9496551711704323 - Acc: 0.95\n",
      "Epoch: #524 - Loss: 0.9494790520827386 - Acc: 0.95\n",
      "Epoch: #525 - Loss: 0.94930329536026 - Acc: 0.95\n",
      "Epoch: #526 - Loss: 0.9491278997497742 - Acc: 0.95\n",
      "Epoch: #527 - Loss: 0.948952864003811 - Acc: 0.95\n",
      "Epoch: #528 - Loss: 0.9487781868806299 - Acc: 0.95\n",
      "Epoch: #529 - Loss: 0.9486038671441988 - Acc: 0.95\n",
      "Epoch: #530 - Loss: 0.9484299035641712 - Acc: 0.95\n",
      "Epoch: #531 - Loss: 0.9482562949158656 - Acc: 0.95\n",
      "Epoch: #532 - Loss: 0.9480830399802423 - Acc: 0.95\n",
      "Epoch: #533 - Loss: 0.9479101375438821 - Acc: 0.95\n",
      "Epoch: #534 - Loss: 0.9477375863989627 - Acc: 0.95\n",
      "Epoch: #535 - Loss: 0.9475653853432372 - Acc: 0.95\n",
      "Epoch: #536 - Loss: 0.9473935331800111 - Acc: 0.95\n",
      "Epoch: #537 - Loss: 0.9472220287181199 - Acc: 0.95\n",
      "Epoch: #538 - Loss: 0.9470508707719056 - Acc: 0.95\n",
      "Epoch: #539 - Loss: 0.9468800581611944 - Acc: 0.95\n",
      "Epoch: #540 - Loss: 0.9467095897112722 - Acc: 0.95\n",
      "Epoch: #541 - Loss: 0.9465394642528634 - Acc: 0.95\n",
      "Epoch: #542 - Loss: 0.946369680622106 - Acc: 0.95\n",
      "Epoch: #543 - Loss: 0.9462002376605301 - Acc: 0.95\n",
      "Epoch: #544 - Loss: 0.9460311342150318 - Acc: 0.95\n",
      "Epoch: #545 - Loss: 0.9458623691378523 - Acc: 0.95\n",
      "Epoch: #546 - Loss: 0.9456939412865525 - Acc: 0.95\n",
      "Epoch: #547 - Loss: 0.9455258495239905 - Acc: 0.95\n",
      "Epoch: #548 - Loss: 0.9453580927182978 - Acc: 0.95\n",
      "Epoch: #549 - Loss: 0.9451906697428548 - Acc: 0.95\n",
      "Epoch: #550 - Loss: 0.9450235794762673 - Acc: 0.95\n",
      "Epoch: #551 - Loss: 0.9448568208023433 - Acc: 0.95\n",
      "Epoch: #552 - Loss: 0.9446903926100686 - Acc: 0.95\n",
      "Epoch: #553 - Loss: 0.9445242937935828 - Acc: 0.95\n",
      "Epoch: #554 - Loss: 0.9443585232521563 - Acc: 0.95\n",
      "Epoch: #555 - Loss: 0.9441930798901641 - Acc: 0.95\n",
      "Epoch: #556 - Loss: 0.9440279626170646 - Acc: 0.95\n",
      "Epoch: #557 - Loss: 0.9438631703473739 - Acc: 0.95\n",
      "Epoch: #558 - Loss: 0.943698702000642 - Acc: 0.95\n",
      "Epoch: #559 - Loss: 0.943534556501429 - Acc: 0.95\n",
      "Epoch: #560 - Loss: 0.9433707327792816 - Acc: 0.95\n",
      "Epoch: #561 - Loss: 0.9432072297687083 - Acc: 0.95\n",
      "Epoch: #562 - Loss: 0.9430440464091547 - Acc: 0.95\n",
      "Epoch: #563 - Loss: 0.942881181644981 - Acc: 0.95\n",
      "Epoch: #564 - Loss: 0.942718634425438 - Acc: 0.95\n",
      "Epoch: #565 - Loss: 0.9425564037046414 - Acc: 0.95\n",
      "Epoch: #566 - Loss: 0.9423944884415486 - Acc: 0.95\n",
      "Epoch: #567 - Loss: 0.942232887599937 - Acc: 0.95\n",
      "Epoch: #568 - Loss: 0.9420716001483747 - Acc: 0.95\n",
      "Epoch: #569 - Loss: 0.9419106250602024 - Acc: 0.95\n",
      "Epoch: #570 - Loss: 0.9417499613135061 - Acc: 0.95\n",
      "Epoch: #571 - Loss: 0.9415896078910941 - Acc: 0.95\n",
      "Epoch: #572 - Loss: 0.941429563780473 - Acc: 0.95\n",
      "Epoch: #573 - Loss: 0.9412698279738236 - Acc: 0.95\n",
      "Epoch: #574 - Loss: 0.9411103994679786 - Acc: 0.95\n",
      "Epoch: #575 - Loss: 0.940951277264396 - Acc: 0.95\n",
      "Epoch: #576 - Loss: 0.9407924603691387 - Acc: 0.95\n",
      "Epoch: #577 - Loss: 0.9406339477928486 - Acc: 0.95\n",
      "Epoch: #578 - Loss: 0.9404757385507232 - Acc: 0.95\n",
      "Epoch: #579 - Loss: 0.9403178316624934 - Acc: 0.95\n",
      "Epoch: #580 - Loss: 0.9401602261523984 - Acc: 0.95\n",
      "Epoch: #581 - Loss: 0.9400029210491629 - Acc: 0.95\n",
      "Epoch: #582 - Loss: 0.9398459153859741 - Acc: 0.95\n",
      "Epoch: #583 - Loss: 0.9396892082004565 - Acc: 0.95\n",
      "Epoch: #584 - Loss: 0.9395327985346527 - Acc: 0.95\n",
      "Epoch: #585 - Loss: 0.9393766854349948 - Acc: 0.95\n",
      "Epoch: #586 - Loss: 0.9392208679522848 - Acc: 0.95\n",
      "Epoch: #587 - Loss: 0.9390653451416707 - Acc: 0.95\n",
      "Epoch: #588 - Loss: 0.9389101160626235 - Acc: 0.95\n",
      "Epoch: #589 - Loss: 0.9387551797789135 - Acc: 0.95\n",
      "Epoch: #590 - Loss: 0.9386005353585877 - Acc: 0.95\n",
      "Epoch: #591 - Loss: 0.9384461818739483 - Acc: 0.95\n",
      "Epoch: #592 - Loss: 0.9382921184015273 - Acc: 0.95\n",
      "Epoch: #593 - Loss: 0.9381383440220656 - Acc: 0.95\n",
      "Epoch: #594 - Loss: 0.9379848578204911 - Acc: 0.95\n",
      "Epoch: #595 - Loss: 0.9378316588858936 - Acc: 0.95\n",
      "Epoch: #596 - Loss: 0.9376787463115047 - Acc: 0.95\n",
      "Epoch: #597 - Loss: 0.9375261191946739 - Acc: 0.95\n",
      "Epoch: #598 - Loss: 0.937373776636847 - Acc: 0.95\n",
      "Epoch: #599 - Loss: 0.9372217177435438 - Acc: 0.95\n",
      "Epoch: #600 - Loss: 0.937069941624336 - Acc: 0.95\n",
      "Epoch: #601 - Loss: 0.9369184473928241 - Acc: 0.95\n",
      "Epoch: #602 - Loss: 0.9367672341666169 - Acc: 0.95\n",
      "Epoch: #603 - Loss: 0.9366163010673085 - Acc: 0.95\n",
      "Epoch: #604 - Loss: 0.9364656472204577 - Acc: 0.95\n",
      "Epoch: #605 - Loss: 0.9363152717555643 - Acc: 0.95\n",
      "Epoch: #606 - Loss: 0.9361651738060494 - Acc: 0.95\n",
      "Epoch: #607 - Loss: 0.936015352509232 - Acc: 0.95\n",
      "Epoch: #608 - Loss: 0.9358658070063092 - Acc: 0.95\n",
      "Epoch: #609 - Loss: 0.9357165364423343 - Acc: 0.95\n",
      "Epoch: #610 - Loss: 0.9355675399661948 - Acc: 0.95\n",
      "Epoch: #611 - Loss: 0.9354188167305915 - Acc: 0.95\n",
      "Epoch: #612 - Loss: 0.9352703658920185 - Acc: 0.94\n",
      "Epoch: #613 - Loss: 0.9351221866107399 - Acc: 0.94\n",
      "Epoch: #614 - Loss: 0.9349742780507709 - Acc: 0.94\n",
      "Epoch: #615 - Loss: 0.9348266393798564 - Acc: 0.94\n",
      "Epoch: #616 - Loss: 0.9346792697694495 - Acc: 0.94\n",
      "Epoch: #617 - Loss: 0.9345321683946914 - Acc: 0.94\n",
      "Epoch: #618 - Loss: 0.9343853344343915 - Acc: 0.94\n",
      "Epoch: #619 - Loss: 0.9342387670710055 - Acc: 0.94\n",
      "Epoch: #620 - Loss: 0.9340924654906164 - Acc: 0.94\n",
      "Epoch: #621 - Loss: 0.9339464288829129 - Acc: 0.94\n",
      "Epoch: #622 - Loss: 0.9338006564411702 - Acc: 0.94\n",
      "Epoch: #623 - Loss: 0.93365514736223 - Acc: 0.94\n",
      "Epoch: #624 - Loss: 0.9335099008464799 - Acc: 0.94\n",
      "Epoch: #625 - Loss: 0.9333649160978332 - Acc: 0.94\n",
      "Epoch: #626 - Loss: 0.933220192323711 - Acc: 0.94\n",
      "Epoch: #627 - Loss: 0.9330757287350193 - Acc: 0.94\n",
      "Epoch: #628 - Loss: 0.9329315245461326 - Acc: 0.94\n",
      "Epoch: #629 - Loss: 0.9327875789748729 - Acc: 0.94\n",
      "Epoch: #630 - Loss: 0.9326438912424897 - Acc: 0.94\n",
      "Epoch: #631 - Loss: 0.9325004605736421 - Acc: 0.93\n",
      "Epoch: #632 - Loss: 0.9323572861963788 - Acc: 0.94\n",
      "Epoch: #633 - Loss: 0.9322143673421196 - Acc: 0.94\n",
      "Epoch: #634 - Loss: 0.9320717032456346 - Acc: 0.94\n",
      "Epoch: #635 - Loss: 0.9319292931450275 - Acc: 0.94\n",
      "Epoch: #636 - Loss: 0.9317871362817167 - Acc: 0.94\n",
      "Epoch: #637 - Loss: 0.9316452319004143 - Acc: 0.94\n",
      "Epoch: #638 - Loss: 0.9315035792491094 - Acc: 0.94\n",
      "Epoch: #639 - Loss: 0.9313621775790502 - Acc: 0.94\n",
      "Epoch: #640 - Loss: 0.9312210261447232 - Acc: 0.94\n",
      "Epoch: #641 - Loss: 0.9310801242038366 - Acc: 0.94\n",
      "Epoch: #642 - Loss: 0.9309394710173026 - Acc: 0.94\n",
      "Epoch: #643 - Loss: 0.9307990658492179 - Acc: 0.94\n",
      "Epoch: #644 - Loss: 0.9306589079668455 - Acc: 0.94\n",
      "Epoch: #645 - Loss: 0.9305189966405991 - Acc: 0.94\n",
      "Epoch: #646 - Loss: 0.9303793311440218 - Acc: 0.94\n",
      "Epoch: #647 - Loss: 0.930239910753772 - Acc: 0.94\n",
      "Epoch: #648 - Loss: 0.9301007347496038 - Acc: 0.94\n",
      "Epoch: #649 - Loss: 0.9299618024143482 - Acc: 0.94\n",
      "Epoch: #650 - Loss: 0.9298231130338994 - Acc: 0.94\n",
      "Epoch: #651 - Loss: 0.9296846658971939 - Acc: 0.94\n",
      "Epoch: #652 - Loss: 0.9295464602961954 - Acc: 0.94\n",
      "Epoch: #653 - Loss: 0.9294084955258768 - Acc: 0.94\n",
      "Epoch: #654 - Loss: 0.9292707708842033 - Acc: 0.94\n",
      "Epoch: #655 - Loss: 0.9291332856721158 - Acc: 0.94\n",
      "Epoch: #656 - Loss: 0.9289960391935135 - Acc: 0.94\n",
      "Epoch: #657 - Loss: 0.9288590307552378 - Acc: 0.94\n",
      "Epoch: #658 - Loss: 0.9287222596670553 - Acc: 0.94\n",
      "Epoch: #659 - Loss: 0.9285857252416415 - Acc: 0.93\n",
      "Epoch: #660 - Loss: 0.9284494267945638 - Acc: 0.93\n",
      "Epoch: #661 - Loss: 0.9283133636442661 - Acc: 0.93\n",
      "Epoch: #662 - Loss: 0.9281775351120518 - Acc: 0.93\n",
      "Epoch: #663 - Loss: 0.9280419405220683 - Acc: 0.93\n",
      "Epoch: #664 - Loss: 0.9279065792012894 - Acc: 0.93\n",
      "Epoch: #665 - Loss: 0.9277714504795017 - Acc: 0.93\n",
      "Epoch: #666 - Loss: 0.9276365536892874 - Acc: 0.93\n",
      "Epoch: #667 - Loss: 0.9275018881660082 - Acc: 0.93\n",
      "Epoch: #668 - Loss: 0.9273674532477907 - Acc: 0.93\n",
      "Epoch: #669 - Loss: 0.9272332482755095 - Acc: 0.93\n",
      "Epoch: #670 - Loss: 0.9270992725927736 - Acc: 0.93\n",
      "Epoch: #671 - Loss: 0.9269655255459088 - Acc: 0.93\n",
      "Epoch: #672 - Loss: 0.9268320064839444 - Acc: 0.93\n",
      "Epoch: #673 - Loss: 0.9266987147585962 - Acc: 0.93\n",
      "Epoch: #674 - Loss: 0.9265656497242537 - Acc: 0.93\n",
      "Epoch: #675 - Loss: 0.9264328107379624 - Acc: 0.93\n",
      "Epoch: #676 - Loss: 0.9263001971594107 - Acc: 0.93\n",
      "Epoch: #677 - Loss: 0.9261678083509146 - Acc: 0.93\n",
      "Epoch: #678 - Loss: 0.9260356436774035 - Acc: 0.93\n",
      "Epoch: #679 - Loss: 0.9259037025064042 - Acc: 0.93\n",
      "Epoch: #680 - Loss: 0.9257719842080273 - Acc: 0.93\n",
      "Epoch: #681 - Loss: 0.9256404881549533 - Acc: 0.93\n",
      "Epoch: #682 - Loss: 0.9255092137224168 - Acc: 0.93\n",
      "Epoch: #683 - Loss: 0.9253781602881933 - Acc: 0.93\n",
      "Epoch: #684 - Loss: 0.9252473272325848 - Acc: 0.93\n",
      "Epoch: #685 - Loss: 0.9251167139384056 - Acc: 0.93\n",
      "Epoch: #686 - Loss: 0.9249863197909679 - Acc: 0.93\n",
      "Epoch: #687 - Loss: 0.9248561441780692 - Acc: 0.93\n",
      "Epoch: #688 - Loss: 0.9247261864899758 - Acc: 0.93\n",
      "Epoch: #689 - Loss: 0.9245964461194129 - Acc: 0.93\n",
      "Epoch: #690 - Loss: 0.924466922461548 - Acc: 0.93\n",
      "Epoch: #691 - Loss: 0.9243376149139771 - Acc: 0.93\n",
      "Epoch: #692 - Loss: 0.924208522876714 - Acc: 0.93\n",
      "Epoch: #693 - Loss: 0.9240796457521742 - Acc: 0.93\n",
      "Epoch: #694 - Loss: 0.9239509829451629 - Acc: 0.93\n",
      "Epoch: #695 - Loss: 0.9238225338628613 - Acc: 0.93\n",
      "Epoch: #696 - Loss: 0.9236942979148138 - Acc: 0.93\n",
      "Epoch: #697 - Loss: 0.9235662745129151 - Acc: 0.93\n",
      "Epoch: #698 - Loss: 0.9234384630713959 - Acc: 0.93\n",
      "Epoch: #699 - Loss: 0.9233108630068118 - Acc: 0.93\n",
      "Epoch: #700 - Loss: 0.9231834737380297 - Acc: 0.93\n",
      "Epoch: #701 - Loss: 0.9230562946862145 - Acc: 0.93\n",
      "Epoch: #702 - Loss: 0.9229293252748179 - Acc: 0.93\n",
      "Epoch: #703 - Loss: 0.9228025649295645 - Acc: 0.93\n",
      "Epoch: #704 - Loss: 0.922676013078439 - Acc: 0.93\n",
      "Epoch: #705 - Loss: 0.9225496691516768 - Acc: 0.93\n",
      "Epoch: #706 - Loss: 0.9224235325817471 - Acc: 0.93\n",
      "Epoch: #707 - Loss: 0.9222976028033444 - Acc: 0.93\n",
      "Epoch: #708 - Loss: 0.9221718792533744 - Acc: 0.93\n",
      "Epoch: #709 - Loss: 0.9220463613709433 - Acc: 0.93\n",
      "Epoch: #710 - Loss: 0.9219210485973441 - Acc: 0.93\n",
      "Epoch: #711 - Loss: 0.9217959403760464 - Acc: 0.93\n",
      "Epoch: #712 - Loss: 0.9216710361526835 - Acc: 0.93\n",
      "Epoch: #713 - Loss: 0.92154633537504 - Acc: 0.93\n",
      "Epoch: #714 - Loss: 0.9214218374930427 - Acc: 0.93\n",
      "Epoch: #715 - Loss: 0.9212975419587462 - Acc: 0.93\n",
      "Epoch: #716 - Loss: 0.9211734482263227 - Acc: 0.93\n",
      "Epoch: #717 - Loss: 0.9210495557520502 - Acc: 0.93\n",
      "Epoch: #718 - Loss: 0.9209258639943014 - Acc: 0.93\n",
      "Epoch: #719 - Loss: 0.9208023724135322 - Acc: 0.93\n",
      "Epoch: #720 - Loss: 0.9206790804722702 - Acc: 0.93\n",
      "Epoch: #721 - Loss: 0.9205559876351049 - Acc: 0.93\n",
      "Epoch: #722 - Loss: 0.9204330933686738 - Acc: 0.93\n",
      "Epoch: #723 - Loss: 0.9203103971416544 - Acc: 0.93\n",
      "Epoch: #724 - Loss: 0.9201878984247522 - Acc: 0.93\n",
      "Epoch: #725 - Loss: 0.9200655966906883 - Acc: 0.93\n",
      "Epoch: #726 - Loss: 0.919943491414191 - Acc: 0.93\n",
      "Epoch: #727 - Loss: 0.9198215820719834 - Acc: 0.93\n",
      "Epoch: #728 - Loss: 0.9196998681427744 - Acc: 0.93\n",
      "Epoch: #729 - Loss: 0.9195783491072452 - Acc: 0.93\n",
      "Epoch: #730 - Loss: 0.9194570244480424 - Acc: 0.93\n",
      "Epoch: #731 - Loss: 0.9193358936497651 - Acc: 0.93\n",
      "Epoch: #732 - Loss: 0.9192149561989551 - Acc: 0.93\n",
      "Epoch: #733 - Loss: 0.9190942115840869 - Acc: 0.93\n",
      "Epoch: #734 - Loss: 0.9189736592955573 - Acc: 0.93\n",
      "Epoch: #735 - Loss: 0.9188532988256751 - Acc: 0.93\n",
      "Epoch: #736 - Loss: 0.9187331296686507 - Acc: 0.93\n",
      "Epoch: #737 - Loss: 0.9186131513205877 - Acc: 0.93\n",
      "Epoch: #738 - Loss: 0.9184933632794703 - Acc: 0.93\n",
      "Epoch: #739 - Loss: 0.9183737650451556 - Acc: 0.93\n",
      "Epoch: #740 - Loss: 0.9182543561193626 - Acc: 0.93\n",
      "Epoch: #741 - Loss: 0.9181351360056631 - Acc: 0.93\n",
      "Epoch: #742 - Loss: 0.9180161042094712 - Acc: 0.93\n",
      "Epoch: #743 - Loss: 0.9178972602380345 - Acc: 0.93\n",
      "Epoch: #744 - Loss: 0.9177786036004242 - Acc: 0.93\n",
      "Epoch: #745 - Loss: 0.9176601338075252 - Acc: 0.93\n",
      "Epoch: #746 - Loss: 0.9175418503720272 - Acc: 0.93\n",
      "Epoch: #747 - Loss: 0.9174237528084147 - Acc: 0.93\n",
      "Epoch: #748 - Loss: 0.9173058406329588 - Acc: 0.93\n",
      "Epoch: #749 - Loss: 0.9171881133637066 - Acc: 0.93\n",
      "Epoch: #750 - Loss: 0.9170705705204729 - Acc: 0.93\n",
      "Epoch: #751 - Loss: 0.9169532116248301 - Acc: 0.93\n",
      "Epoch: #752 - Loss: 0.9168360362001007 - Acc: 0.93\n",
      "Epoch: #753 - Loss: 0.9167190437713464 - Acc: 0.93\n",
      "Epoch: #754 - Loss: 0.9166022338653609 - Acc: 0.93\n",
      "Epoch: #755 - Loss: 0.9164856060106594 - Acc: 0.93\n",
      "Epoch: #756 - Loss: 0.9163691597374714 - Acc: 0.93\n",
      "Epoch: #757 - Loss: 0.9162528945777296 - Acc: 0.93\n",
      "Epoch: #758 - Loss: 0.9161368100650645 - Acc: 0.93\n",
      "Epoch: #759 - Loss: 0.9160209057347922 - Acc: 0.93\n",
      "Epoch: #760 - Loss: 0.9159051811239084 - Acc: 0.93\n",
      "Epoch: #761 - Loss: 0.9157896357710779 - Acc: 0.93\n",
      "Epoch: #762 - Loss: 0.9156742692166281 - Acc: 0.93\n",
      "Epoch: #763 - Loss: 0.9155590810025387 - Acc: 0.93\n",
      "Epoch: #764 - Loss: 0.9154440706724348 - Acc: 0.93\n",
      "Epoch: #765 - Loss: 0.9153292377715775 - Acc: 0.93\n",
      "Epoch: #766 - Loss: 0.9152145818468557 - Acc: 0.93\n",
      "Epoch: #767 - Loss: 0.9151001024467792 - Acc: 0.93\n",
      "Epoch: #768 - Loss: 0.9149857991214677 - Acc: 0.92\n",
      "Epoch: #769 - Loss: 0.9148716714226471 - Acc: 0.92\n",
      "Epoch: #770 - Loss: 0.9147577189036364 - Acc: 0.92\n",
      "Epoch: #771 - Loss: 0.9146439411193432 - Acc: 0.92\n",
      "Epoch: #772 - Loss: 0.9145303376262547 - Acc: 0.92\n",
      "Epoch: #773 - Loss: 0.9144169079824297 - Acc: 0.92\n",
      "Epoch: #774 - Loss: 0.9143036517474907 - Acc: 0.92\n",
      "Epoch: #775 - Loss: 0.9141905684826167 - Acc: 0.92\n",
      "Epoch: #776 - Loss: 0.9140776577505341 - Acc: 0.92\n",
      "Epoch: #777 - Loss: 0.9139649191155104 - Acc: 0.92\n",
      "Epoch: #778 - Loss: 0.9138523521433456 - Acc: 0.92\n",
      "Epoch: #779 - Loss: 0.9137399564013658 - Acc: 0.92\n",
      "Epoch: #780 - Loss: 0.9136277314584148 - Acc: 0.92\n",
      "Epoch: #781 - Loss: 0.9135156768848456 - Acc: 0.92\n",
      "Epoch: #782 - Loss: 0.913403792252515 - Acc: 0.92\n",
      "Epoch: #783 - Loss: 0.9132920771347748 - Acc: 0.92\n",
      "Epoch: #784 - Loss: 0.9131805311064655 - Acc: 0.92\n",
      "Epoch: #785 - Loss: 0.9130691537439075 - Acc: 0.92\n",
      "Epoch: #786 - Loss: 0.9129579446248954 - Acc: 0.92\n",
      "Epoch: #787 - Loss: 0.9128469033286896 - Acc: 0.92\n",
      "Epoch: #788 - Loss: 0.9127360294360096 - Acc: 0.92\n",
      "Epoch: #789 - Loss: 0.9126253225290282 - Acc: 0.92\n",
      "Epoch: #790 - Loss: 0.912514782191361 - Acc: 0.92\n",
      "Epoch: #791 - Loss: 0.9124044080080633 - Acc: 0.92\n",
      "Epoch: #792 - Loss: 0.912294199565621 - Acc: 0.92\n",
      "Epoch: #793 - Loss: 0.9121841564519431 - Acc: 0.92\n",
      "Epoch: #794 - Loss: 0.9120742782563567 - Acc: 0.92\n",
      "Epoch: #795 - Loss: 0.9119645645695986 - Acc: 0.92\n",
      "Epoch: #796 - Loss: 0.9118550149838103 - Acc: 0.91\n",
      "Epoch: #797 - Loss: 0.9117456290925281 - Acc: 0.91\n",
      "Epoch: #798 - Loss: 0.9116364064906797 - Acc: 0.91\n",
      "Epoch: #799 - Loss: 0.9115273467745756 - Acc: 0.91\n",
      "Epoch: #800 - Loss: 0.9114184495419037 - Acc: 0.91\n",
      "Epoch: #801 - Loss: 0.9113097143917211 - Acc: 0.91\n",
      "Epoch: #802 - Loss: 0.9112011409244493 - Acc: 0.91\n",
      "Epoch: #803 - Loss: 0.9110927287418663 - Acc: 0.91\n",
      "Epoch: #804 - Loss: 0.9109844774471014 - Acc: 0.91\n",
      "Epoch: #805 - Loss: 0.9108763866446278 - Acc: 0.91\n",
      "Epoch: #806 - Loss: 0.9107684559402561 - Acc: 0.91\n",
      "Epoch: #807 - Loss: 0.9106606849411291 - Acc: 0.91\n",
      "Epoch: #808 - Loss: 0.9105530732557149 - Acc: 0.91\n",
      "Epoch: #809 - Loss: 0.9104456204938 - Acc: 0.91\n",
      "Epoch: #810 - Loss: 0.9103383262664845 - Acc: 0.91\n",
      "Epoch: #811 - Loss: 0.9102311901861738 - Acc: 0.91\n",
      "Epoch: #812 - Loss: 0.9101242118665751 - Acc: 0.91\n",
      "Epoch: #813 - Loss: 0.9100173909226893 - Acc: 0.91\n",
      "Epoch: #814 - Loss: 0.9099107269708058 - Acc: 0.91\n",
      "Epoch: #815 - Loss: 0.9098042196284962 - Acc: 0.91\n",
      "Epoch: #816 - Loss: 0.9096978685146091 - Acc: 0.91\n",
      "Epoch: #817 - Loss: 0.9095916732492617 - Acc: 0.91\n",
      "Epoch: #818 - Loss: 0.9094856334538379 - Acc: 0.91\n",
      "Epoch: #819 - Loss: 0.9093797487509789 - Acc: 0.91\n",
      "Epoch: #820 - Loss: 0.9092740187645787 - Acc: 0.91\n",
      "Epoch: #821 - Loss: 0.909168443119779 - Acc: 0.91\n",
      "Epoch: #822 - Loss: 0.9090630214429619 - Acc: 0.91\n",
      "Epoch: #823 - Loss: 0.9089577533617456 - Acc: 0.91\n",
      "Epoch: #824 - Loss: 0.9088526385049778 - Acc: 0.91\n",
      "Epoch: #825 - Loss: 0.9087476765027305 - Acc: 0.91\n",
      "Epoch: #826 - Loss: 0.9086428669862945 - Acc: 0.91\n",
      "Epoch: #827 - Loss: 0.9085382095881734 - Acc: 0.91\n",
      "Epoch: #828 - Loss: 0.908433703942078 - Acc: 0.91\n",
      "Epoch: #829 - Loss: 0.9083293496829216 - Acc: 0.91\n",
      "Epoch: #830 - Loss: 0.9082251464468133 - Acc: 0.91\n",
      "Epoch: #831 - Loss: 0.9081210938710537 - Acc: 0.91\n",
      "Epoch: #832 - Loss: 0.9080171915941291 - Acc: 0.91\n",
      "Epoch: #833 - Loss: 0.9079134392557058 - Acc: 0.91\n",
      "Epoch: #834 - Loss: 0.9078098364966248 - Acc: 0.91\n",
      "Epoch: #835 - Loss: 0.9077063829588976 - Acc: 0.91\n",
      "Epoch: #836 - Loss: 0.9076030782856984 - Acc: 0.91\n",
      "Epoch: #837 - Loss: 0.9074999221213624 - Acc: 0.91\n",
      "Epoch: #838 - Loss: 0.9073969141113776 - Acc: 0.91\n",
      "Epoch: #839 - Loss: 0.907294053902381 - Acc: 0.91\n",
      "Epoch: #840 - Loss: 0.9071913411421523 - Acc: 0.91\n",
      "Epoch: #841 - Loss: 0.9070887754796111 - Acc: 0.91\n",
      "Epoch: #842 - Loss: 0.9069863565648097 - Acc: 0.91\n",
      "Epoch: #843 - Loss: 0.9068840840489285 - Acc: 0.91\n",
      "Epoch: #844 - Loss: 0.9067819575842714 - Acc: 0.91\n",
      "Epoch: #845 - Loss: 0.9066799768242606 - Acc: 0.91\n",
      "Epoch: #846 - Loss: 0.9065781414234323 - Acc: 0.91\n",
      "Epoch: #847 - Loss: 0.9064764510374292 - Acc: 0.91\n",
      "Epoch: #848 - Loss: 0.906374905323 - Acc: 0.91\n",
      "Epoch: #849 - Loss: 0.9062735039379902 - Acc: 0.91\n",
      "Epoch: #850 - Loss: 0.9061722465413405 - Acc: 0.91\n",
      "Epoch: #851 - Loss: 0.9060711327930794 - Acc: 0.91\n",
      "Epoch: #852 - Loss: 0.90597016235432 - Acc: 0.91\n",
      "Epoch: #853 - Loss: 0.9058693348872555 - Acc: 0.91\n",
      "Epoch: #854 - Loss: 0.9057686500551533 - Acc: 0.91\n",
      "Epoch: #855 - Loss: 0.9056681075223506 - Acc: 0.91\n",
      "Epoch: #856 - Loss: 0.905567706954251 - Acc: 0.91\n",
      "Epoch: #857 - Loss: 0.9054674480173177 - Acc: 0.91\n",
      "Epoch: #858 - Loss: 0.905367330379071 - Acc: 0.91\n",
      "Epoch: #859 - Loss: 0.905267353708082 - Acc: 0.91\n",
      "Epoch: #860 - Loss: 0.9051675176739692 - Acc: 0.91\n",
      "Epoch: #861 - Loss: 0.9050678219473934 - Acc: 0.91\n",
      "Epoch: #862 - Loss: 0.9049682662000543 - Acc: 0.91\n",
      "Epoch: #863 - Loss: 0.9048688501046835 - Acc: 0.91\n",
      "Epoch: #864 - Loss: 0.9047695733350429 - Acc: 0.91\n",
      "Epoch: #865 - Loss: 0.9046704355659184 - Acc: 0.91\n",
      "Epoch: #866 - Loss: 0.9045714364731168 - Acc: 0.91\n",
      "Epoch: #867 - Loss: 0.9044725757334598 - Acc: 0.91\n",
      "Epoch: #868 - Loss: 0.9043738530247822 - Acc: 0.91\n",
      "Epoch: #869 - Loss: 0.9042752680259237 - Acc: 0.91\n",
      "Epoch: #870 - Loss: 0.9041768204167294 - Acc: 0.91\n",
      "Epoch: #871 - Loss: 0.9040785098780411 - Acc: 0.91\n",
      "Epoch: #872 - Loss: 0.9039803360916968 - Acc: 0.91\n",
      "Epoch: #873 - Loss: 0.9038822987405228 - Acc: 0.91\n",
      "Epoch: #874 - Loss: 0.9037843975083327 - Acc: 0.91\n",
      "Epoch: #875 - Loss: 0.9036866320799216 - Acc: 0.91\n",
      "Epoch: #876 - Loss: 0.9035890021410622 - Acc: 0.91\n",
      "Epoch: #877 - Loss: 0.9034915073785014 - Acc: 0.91\n",
      "Epoch: #878 - Loss: 0.9033941474799542 - Acc: 0.91\n",
      "Epoch: #879 - Loss: 0.9032969221341021 - Acc: 0.91\n",
      "Epoch: #880 - Loss: 0.903199831030588 - Acc: 0.91\n",
      "Epoch: #881 - Loss: 0.903102873860012 - Acc: 0.91\n",
      "Epoch: #882 - Loss: 0.9030060503139269 - Acc: 0.91\n",
      "Epoch: #883 - Loss: 0.902909360084836 - Acc: 0.91\n",
      "Epoch: #884 - Loss: 0.9028128028661864 - Acc: 0.91\n",
      "Epoch: #885 - Loss: 0.9027163783523683 - Acc: 0.91\n",
      "Epoch: #886 - Loss: 0.9026200862387088 - Acc: 0.91\n",
      "Epoch: #887 - Loss: 0.9025239262214685 - Acc: 0.91\n",
      "Epoch: #888 - Loss: 0.9024278979978381 - Acc: 0.91\n",
      "Epoch: #889 - Loss: 0.9023320012659342 - Acc: 0.91\n",
      "Epoch: #890 - Loss: 0.902236235724795 - Acc: 0.91\n",
      "Epoch: #891 - Loss: 0.9021406010743783 - Acc: 0.91\n",
      "Epoch: #892 - Loss: 0.9020450970155548 - Acc: 0.91\n",
      "Epoch: #893 - Loss: 0.9019497232501075 - Acc: 0.91\n",
      "Epoch: #894 - Loss: 0.901854479480726 - Acc: 0.91\n",
      "Epoch: #895 - Loss: 0.9017593654110027 - Acc: 0.91\n",
      "Epoch: #896 - Loss: 0.9016643807454306 - Acc: 0.91\n",
      "Epoch: #897 - Loss: 0.9015695251893976 - Acc: 0.91\n",
      "Epoch: #898 - Loss: 0.9014747984491845 - Acc: 0.91\n",
      "Epoch: #899 - Loss: 0.9013802002319619 - Acc: 0.91\n",
      "Epoch: #900 - Loss: 0.9012857302457832 - Acc: 0.91\n",
      "Epoch: #901 - Loss: 0.9011913881995851 - Acc: 0.91\n",
      "Epoch: #902 - Loss: 0.9010971738031818 - Acc: 0.91\n",
      "Epoch: #903 - Loss: 0.9010030867672615 - Acc: 0.91\n",
      "Epoch: #904 - Loss: 0.9009091268033839 - Acc: 0.91\n",
      "Epoch: #905 - Loss: 0.9008152936239755 - Acc: 0.91\n",
      "Epoch: #906 - Loss: 0.9007215869423264 - Acc: 0.91\n",
      "Epoch: #907 - Loss: 0.9006280064725881 - Acc: 0.91\n",
      "Epoch: #908 - Loss: 0.9005345519297682 - Acc: 0.91\n",
      "Epoch: #909 - Loss: 0.9004412230297281 - Acc: 0.91\n",
      "Epoch: #910 - Loss: 0.900348019489179 - Acc: 0.91\n",
      "Epoch: #911 - Loss: 0.9002549410256796 - Acc: 0.91\n",
      "Epoch: #912 - Loss: 0.9001619873576305 - Acc: 0.91\n",
      "Epoch: #913 - Loss: 0.9000691582042737 - Acc: 0.91\n",
      "Epoch: #914 - Loss: 0.8999764532856864 - Acc: 0.91\n",
      "Epoch: #915 - Loss: 0.8998838723227803 - Acc: 0.91\n",
      "Epoch: #916 - Loss: 0.8997914150372968 - Acc: 0.91\n",
      "Epoch: #917 - Loss: 0.8996990811518034 - Acc: 0.91\n",
      "Epoch: #918 - Loss: 0.8996068703896912 - Acc: 0.91\n",
      "Epoch: #919 - Loss: 0.8995147824751722 - Acc: 0.91\n",
      "Epoch: #920 - Loss: 0.899422817133275 - Acc: 0.91\n",
      "Epoch: #921 - Loss: 0.8993309740898409 - Acc: 0.91\n",
      "Epoch: #922 - Loss: 0.8992392530715225 - Acc: 0.91\n",
      "Epoch: #923 - Loss: 0.8991476538057804 - Acc: 0.91\n",
      "Epoch: #924 - Loss: 0.8990561760208785 - Acc: 0.91\n",
      "Epoch: #925 - Loss: 0.8989648194458824 - Acc: 0.91\n",
      "Epoch: #926 - Loss: 0.8988735838106542 - Acc: 0.91\n",
      "Epoch: #927 - Loss: 0.8987824688458524 - Acc: 0.91\n",
      "Epoch: #928 - Loss: 0.8986914742829267 - Acc: 0.91\n",
      "Epoch: #929 - Loss: 0.8986005998541148 - Acc: 0.91\n",
      "Epoch: #930 - Loss: 0.8985098452924405 - Acc: 0.91\n",
      "Epoch: #931 - Loss: 0.8984192103317097 - Acc: 0.91\n",
      "Epoch: #932 - Loss: 0.8983286947065087 - Acc: 0.91\n",
      "Epoch: #933 - Loss: 0.8982382981521994 - Acc: 0.91\n",
      "Epoch: #934 - Loss: 0.8981480204049169 - Acc: 0.91\n",
      "Epoch: #935 - Loss: 0.8980578612015677 - Acc: 0.91\n",
      "Epoch: #936 - Loss: 0.8979678202798245 - Acc: 0.91\n",
      "Epoch: #937 - Loss: 0.8978778973781262 - Acc: 0.91\n",
      "Epoch: #938 - Loss: 0.897788092235672 - Acc: 0.91\n",
      "Epoch: #939 - Loss: 0.89769840459242 - Acc: 0.91\n",
      "Epoch: #940 - Loss: 0.8976088341890844 - Acc: 0.91\n",
      "Epoch: #941 - Loss: 0.897519380767133 - Acc: 0.91\n",
      "Epoch: #942 - Loss: 0.8974300440687815 - Acc: 0.91\n",
      "Epoch: #943 - Loss: 0.8973408238369946 - Acc: 0.91\n",
      "Epoch: #944 - Loss: 0.8972517198154807 - Acc: 0.91\n",
      "Epoch: #945 - Loss: 0.8971627317486897 - Acc: 0.91\n",
      "Epoch: #946 - Loss: 0.8970738593818101 - Acc: 0.91\n",
      "Epoch: #947 - Loss: 0.8969851024607661 - Acc: 0.91\n",
      "Epoch: #948 - Loss: 0.8968964607322156 - Acc: 0.91\n",
      "Epoch: #949 - Loss: 0.8968079339435459 - Acc: 0.91\n",
      "Epoch: #950 - Loss: 0.8967195218428723 - Acc: 0.91\n",
      "Epoch: #951 - Loss: 0.8966312241790352 - Acc: 0.91\n",
      "Epoch: #952 - Loss: 0.8965430407015962 - Acc: 0.91\n",
      "Epoch: #953 - Loss: 0.8964549711608369 - Acc: 0.91\n",
      "Epoch: #954 - Loss: 0.8963670153077555 - Acc: 0.91\n",
      "Epoch: #955 - Loss: 0.8962791728940641 - Acc: 0.91\n",
      "Epoch: #956 - Loss: 0.8961914436721857 - Acc: 0.91\n",
      "Epoch: #957 - Loss: 0.8961038273952522 - Acc: 0.91\n",
      "Epoch: #958 - Loss: 0.8960163238171018 - Acc: 0.91\n",
      "Epoch: #959 - Loss: 0.8959289326922749 - Acc: 0.91\n",
      "Epoch: #960 - Loss: 0.895841653776014 - Acc: 0.91\n",
      "Epoch: #961 - Loss: 0.8957544868242587 - Acc: 0.91\n",
      "Epoch: #962 - Loss: 0.8956674315936448 - Acc: 0.91\n",
      "Epoch: #963 - Loss: 0.8955804878414998 - Acc: 0.91\n",
      "Epoch: #964 - Loss: 0.8954936553258434 - Acc: 0.91\n",
      "Epoch: #965 - Loss: 0.8954069338053815 - Acc: 0.91\n",
      "Epoch: #966 - Loss: 0.8953203230395057 - Acc: 0.91\n",
      "Epoch: #967 - Loss: 0.8952338227882904 - Acc: 0.91\n",
      "Epoch: #968 - Loss: 0.8951474328124904 - Acc: 0.91\n",
      "Epoch: #969 - Loss: 0.8950611528735383 - Acc: 0.91\n",
      "Epoch: #970 - Loss: 0.8949749827335406 - Acc: 0.91\n",
      "Epoch: #971 - Loss: 0.8948889221552785 - Acc: 0.91\n",
      "Epoch: #972 - Loss: 0.8948029709022016 - Acc: 0.91\n",
      "Epoch: #973 - Loss: 0.8947171287384286 - Acc: 0.91\n",
      "Epoch: #974 - Loss: 0.8946313954287431 - Acc: 0.91\n",
      "Epoch: #975 - Loss: 0.8945457707385905 - Acc: 0.91\n",
      "Epoch: #976 - Loss: 0.8944602544340788 - Acc: 0.91\n",
      "Epoch: #977 - Loss: 0.8943748462819727 - Acc: 0.91\n",
      "Epoch: #978 - Loss: 0.8942895460496929 - Acc: 0.91\n",
      "Epoch: #979 - Loss: 0.894204353505313 - Acc: 0.91\n",
      "Epoch: #980 - Loss: 0.8941192684175582 - Acc: 0.91\n",
      "Epoch: #981 - Loss: 0.8940342905558016 - Acc: 0.91\n",
      "Epoch: #982 - Loss: 0.8939494196900629 - Acc: 0.91\n",
      "Epoch: #983 - Loss: 0.8938646555910056 - Acc: 0.91\n",
      "Epoch: #984 - Loss: 0.8937799980299349 - Acc: 0.91\n",
      "Epoch: #985 - Loss: 0.8936954467787946 - Acc: 0.91\n",
      "Epoch: #986 - Loss: 0.8936110016101664 - Acc: 0.91\n",
      "Epoch: #987 - Loss: 0.8935266622972655 - Acc: 0.91\n",
      "Epoch: #988 - Loss: 0.8934424286139404 - Acc: 0.91\n",
      "Epoch: #989 - Loss: 0.8933583003346687 - Acc: 0.91\n",
      "Epoch: #990 - Loss: 0.8932742772345569 - Acc: 0.91\n",
      "Epoch: #991 - Loss: 0.8931903590893364 - Acc: 0.91\n",
      "Epoch: #992 - Loss: 0.8931065456753617 - Acc: 0.91\n",
      "Epoch: #993 - Loss: 0.893022836769609 - Acc: 0.91\n",
      "Epoch: #994 - Loss: 0.8929392321496729 - Acc: 0.91\n",
      "Epoch: #995 - Loss: 0.8928557315937646 - Acc: 0.91\n",
      "Epoch: #996 - Loss: 0.8927723348807102 - Acc: 0.91\n",
      "Epoch: #997 - Loss: 0.8926890417899479 - Acc: 0.91\n",
      "Epoch: #998 - Loss: 0.8926058521015258 - Acc: 0.91\n",
      "Epoch: #999 - Loss: 0.8925227655960994 - Acc: 0.91\n"
     ]
    }
   ],
   "source": [
    "class SoftmaxRegressor():\n",
    "\n",
    "    def __init__(self, lr: float) -> None:\n",
    "\n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def fit(self, x: np.ndarray, y: np.ndarray, epochs: int, batch_size: int, early_stopping: int = 0):\n",
    "        \n",
    "        # One-hot encode targets\n",
    "        y_onehot = np.eye(np.max(y) + 1)[y] # NOTE: It's possible to select the same row various times via indexing\n",
    "\n",
    "        # Apparently numpy uses shapes starting from the last dimension (y, x) instead of (x, y)\n",
    "        self.n_classes = y_onehot.shape[1]\n",
    "        self.n_features = x.shape[1]\n",
    "        self.coeffs = np.random.uniform(0, 1, (self.n_features, self.n_classes))\n",
    "        self.biases = np.random.uniform(0, 1, self.n_classes)\n",
    "\n",
    "        epoch_losses = np.array([])\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            batch_losses = np.array([])\n",
    "\n",
    "            for step in range(len(x) // batch_size):\n",
    "\n",
    "                # Set up variables for step\n",
    "                batch = slice(step*batch_size, min((step + 1)*batch_size, len(x)))\n",
    "                samples = x[batch, :]\n",
    "                targets = y_onehot[batch, :]\n",
    "\n",
    "                preds, logits, _ = self.predict(samples, steps=True)\n",
    "                losses = categorical_crossentropy(preds, targets)\n",
    "\n",
    "                # Calculate the derivative of all parameters with respect to all samples in the batch\n",
    "                cee_gradient = -targets / preds\n",
    "                softmax_true_gradient = (targets - preds) * preds\n",
    "                sigmoid_gradient = logits * (np.ones(self.n_classes) - logits)\n",
    "                coeff_derivative = np.repeat(samples[:, :, None], repeats=self.n_classes, axis=2)\n",
    "\n",
    "                biases_gradients = sigmoid_gradient * softmax_true_gradient * cee_gradient[targets == 1, None]\n",
    "                coeffs_gradients = coeff_derivative * biases_gradients[:, None, :]\n",
    "\n",
    "                dbiases = np.mean(biases_gradients, axis=0)\n",
    "                dcoeffs = np.mean(coeffs_gradients, axis=0)\n",
    "\n",
    "                self.coeffs -= dcoeffs * self.learning_rate\n",
    "                self.biases -= dbiases * self.learning_rate\n",
    "\n",
    "                batch_losses = np.append(batch_losses, np.mean(losses))\n",
    "\n",
    "            accuracy = np.sum(np.argmax(self.predict(x), axis=1) == y) / len(y)\n",
    "            epoch_losses = np.append(epoch_losses, np.mean(batch_losses))\n",
    "            print(f\"Epoch: #{epoch} - Loss: {epoch_losses[-1]} - Acc: {accuracy:.2f}\")\n",
    "\n",
    "            if 0 < early_stopping < len(epoch_losses) - np.argmin(epoch_losses):\n",
    "                break\n",
    "\n",
    "    def predict(self, x: np.ndarray, steps: bool = False) -> typing.Union[tuple, np.ndarray]:\n",
    "        coeff_out = x @ self.coeffs + self.biases\n",
    "        logits = sigmoid(coeff_out)\n",
    "        preds = softmax(logits)\n",
    "        return (preds, logits, coeff_out) if steps else preds\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax function.\n",
    "\n",
    "    :param x: 2D input matrix\n",
    "    :type x: np.ndarray\n",
    "    :return out: softmax of x\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    x_exp = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return x_exp / np.sum(x_exp, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sigmoid function.\n",
    "\n",
    "    :param x: 2D input matrix\n",
    "    :type x: np.ndarray\n",
    "    :return out: sigmoid of x\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def categorical_crossentropy(y_pred: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Categorical cross-entropy loss function.\n",
    "\n",
    "    :param y_pred: predicted prob dist 2D matrix\n",
    "    :type y_pred: np.ndarray\n",
    "    :param y_true: true prob dist 2D matrix\n",
    "    :type y_true: np.ndarray\n",
    "    :return out: categorical cross-entropy between y_true and y_pred\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    loss = np.sum(-np.log(y_pred) * y_true, axis=1)\n",
    "    return loss\n",
    "\n",
    "model = SoftmaxRegressor(0.5)\n",
    "model.fit(X, y, 1000, 150, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.91025641, 0.3974359 , 0.75641026, 0.21794872]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[None, 40, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.19564644, 0.39559385, 0.40875971]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[None, 40, :], False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs = np.ones((4, 3))\n",
    "bias = np.ones(3)\n",
    "samples = np.array([[3, 7.1, 2, 5.1], [1.442, 1.2, 4, 0.11]])\n",
    "targets = np.array([[0, 1, 0], [1, 0, 0]])\n",
    "n_classes = 3\n",
    "n_samples = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x: np.ndarray, steps: bool = False) -> typing.Union[tuple, np.ndarray]:\n",
    "    coeff_out = x @ coeffs + bias\n",
    "    logits = sigmoid(coeff_out)\n",
    "    preds = softmax(logits)\n",
    "    return (preds, logits, coeff_out) if steps else preds\n",
    "\n",
    "preds, logits, _ = predict(samples, steps=True)\n",
    "losses = categorical_crossentropy(preds, targets)\n",
    "\n",
    "# cee_gradient = -targets / preds\n",
    "# softmax_jacobian = (np.eye(n_classes) - preds.reshape((n_samples, 1, n_classes))) * preds.reshape((n_samples, n_classes, 1))\n",
    "# sigmoid_gradient = logits * (np.ones(n_classes) - logits)\n",
    "# coeff_derivative = samples\n",
    "\n",
    "cee_gradient = -targets / preds\n",
    "softmax_true_gradient = (targets - preds) * preds\n",
    "sigmoid_gradient = logits * (np.ones(n_classes) - logits)\n",
    "coeff_derivative = np.repeat(samples[:, :, None], repeats=n_classes, axis=2)\n",
    "\n",
    "biases_gradients = sigmoid_gradient * softmax_true_gradient * cee_gradient[targets == 1, None]\n",
    "coeffs_gradients = coeff_derivative * biases_gradients[:, None, :]\n",
    "\n",
    "dbiases = np.mean(biases_gradients, axis=0)\n",
    "dcoeffs = np.mean(coeffs_gradients, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive 3rd coeff from 1st class for the 2nd sample\n",
    "# coeff_derivative[1, 2] * sigmoid_gradient[1, 0] * softmax_jacobian[1, 0, 0] * cee_gradient[1, 0]\n",
    "# All coeffs in the 1st class from the 2nd sample\n",
    "# coeff_derivative[1, :] * sigmoid_gradient[1, 2] * softmax_jacobian[1, 0, 0] * cee_gradient[1, 0]\n",
    "# All coeffs in the 3rd class from the 2nd sample\n",
    "# coeff_derivative[1, :] * sigmoid_gradient[1, 0] * softmax_jacobian[1, 0, 2] * cee_gradient[1, 0]\n",
    "# All coefs in all classes from the 2nd sample (sliced softmax jacobian)\n",
    "# (coeff_derivative[1, :] * coeffs.T).T * sigmoid_gradient[1, :] * softmax_jacobian[1, 0, :] * cee_gradient[1, 0]\n",
    "# All coefs in all classes from the 2nd sample (full softmax jacobian and full CEE gradient)\n",
    "# (coeff_derivative[1, :] * coeffs.T).T * sigmoid_gradient[1, :] * np.dot(softmax_jacobian[1, :, :], cee_gradient[1, :])\n",
    "# All coefs in all classes from the 2nd sample (optimized softmax gradient)\n",
    "# (coeff_derivative[1, :] * coeffs.T).T * sigmoid_gradient[1, :] * softmax_true_gradient[1, :] * cee_gradient[1, 0]\n",
    "# All coeffs in all classes from all samples (method 1)\n",
    "# np.repeat(coeff_derivative[:, :, None], repeats=3, axis=2) * sigmoid_gradient[:, None, :] * softmax_true_gradient[:, None, :] * cee_gradient[[0, 1], [1, 0], None, None]\n",
    "# Bias in all classes from all samples\n",
    "# sigmoid_gradient * softmax_true_gradient * cee_gradient[[0, 1], [1, 0], None]\n",
    "# All coeffs in all classes from all samples (method 2)\n",
    "# np.repeat(samples[:, :, None], repeats=3, axis=2) * (sigmoid_gradient * softmax_true_gradient * cee_gradient[targets == 1, None])[:, None, :]\n",
    "# Bias in all classes from the 2nd sample\n",
    "# sigmoid_gradient[1, :] * softmax_true_gradient[1, :] * cee_gradient[1, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qc_311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
